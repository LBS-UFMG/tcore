{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff8f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d658723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjuntos de features\n",
    "FEATURE_SETS = {\n",
    "    'DS1': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'column_Male', 'age', 'vo2máx'],\n",
    "    'DS2': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'age', 'column_Male', 'vo2máx', 'speed'],\n",
    "    'DS3': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'age', 'column_Male', 'vo2máx', 'speed', 'umidade_absoluta', 'dry_temp', 'wet_temp', 'relative_humidity']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7656467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grades de hiperparâmetros para cada modelo\n",
    "PARAM_GRIDS = {\n",
    "    'Lasso': {\n",
    "        'model__alpha': np.linspace(0.01, 0.1, 10),\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__kernel': ['rbf', 'linear'],\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'model__max_depth': [5, 10, 15, 20, None],\n",
    "        'model__min_samples_leaf': [1, 5, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'model__n_estimators': [100, 200, 400],\n",
    "        'model__max_depth': [5, 10, 15],\n",
    "        'model__min_samples_leaf': [5, 10, 20],\n",
    "        'model__max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
    "        'model__max_samples': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'XGBoostRegressor': {\n",
    "        'model__n_estimators': [300, 500, 700],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__subsample': [0.5, 0.7, 0.9],\n",
    "        'model__colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9ce0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funções para o Fluxo do Modelo ---\n",
    "\n",
    "def load_and_split_data(X_path, y_path, groups_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Carrega os dados e os divide em conjuntos de treino e teste.\n",
    "    Utiliza GroupShuffleSplit para garantir que os grupos não se misturem entre os conjuntos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X = pd.read_csv(X_path)\n",
    "        y = pd.read_csv(y_path)\n",
    "        groups = pd.read_csv(groups_path)['trial_number']\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro: Arquivo não encontrado - {e.filename}. Verifique se os caminhos estão corretos.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx].values.ravel(), y.iloc[test_idx].values.ravel()\n",
    "    groups_train = groups.iloc[train_idx]\n",
    "    groups_test = groups.iloc[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, groups_train, groups_test\n",
    "\n",
    "\n",
    "def run_grid_search(X_train, y_train, groups_train, feature_sets, param_grids):\n",
    "    \"\"\"\n",
    "    Executa a busca em grade para múltiplos modelos, retornando os melhores resultados.\n",
    "    \"\"\"\n",
    "    best_results = []\n",
    "    scoring = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
    "\n",
    "    for model_name, param_grid in param_grids.items():\n",
    "        print(f\"\\n--- Iniciando a Busca em Grade para o modelo: {model_name} ---\")\n",
    "\n",
    "        # Usando o conjunto de features DS2, que foi o melhor do notebook original\n",
    "        feature_list = feature_sets['DS2']\n",
    "        X_train_subset = X_train[feature_list]\n",
    "\n",
    "        # Configura o pipeline com o modelo e o scaler se necessário\n",
    "        if model_name in ['Lasso', 'SVR']:\n",
    "            model = eval(f\"{model_name}(\" + \"random_state=42)\" if model_name != \"SVR\" else f\"{model_name}()\")\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', model)\n",
    "            ])\n",
    "        else:\n",
    "            model = eval(f\"{model_name}(\" + \"random_state=42)\" if model_name != \"SVR\" else f\"{model_name}()\")\n",
    "            pipeline = Pipeline([\n",
    "                ('model', model)\n",
    "            ])\n",
    "\n",
    "        # Execução do GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid=param_grid,\n",
    "            cv=LeaveOneGroupOut(),\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            return_train_score=True\n",
    "        )\n",
    "        grid_search.fit(X_train_subset, y_train, groups=groups_train)\n",
    "\n",
    "        print(f\"Busca em Grade para {model_name} Concluída.\")\n",
    "        print(f\"Melhores hiperparâmetros: {grid_search.best_params_}\")\n",
    "        print(f\"Melhor pontuação (RMSE) na validação cruzada: {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "        best_results.append({\n",
    "            'model': model_name,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': -grid_search.best_score_\n",
    "        })\n",
    "    \n",
    "    return best_results\n",
    "\n",
    "\n",
    "def evaluate_final_model(best_results, X_test, y_test, feature_sets):\n",
    "    \"\"\"\n",
    "    Avalia o melhor modelo de todos os grid searches no conjunto de teste (hold-out).\n",
    "    \"\"\"\n",
    "    # Encontra o modelo com o melhor score geral\n",
    "    best_overall_model_name = min(best_results, key=lambda x: x['best_score'])['model']\n",
    "    best_params = min(best_results, key=lambda x: x['best_score'])['best_params']\n",
    "    best_feature_set = feature_sets['DS2'] # O DS2 foi o melhor no notebook original\n",
    "    X_test_best = X_test[best_feature_set]\n",
    "\n",
    "    # Re-instancia o melhor modelo com os melhores parâmetros\n",
    "    if best_overall_model_name in ['Lasso', 'SVR']:\n",
    "        final_model_instance = eval(f\"{best_overall_model_name}()\")\n",
    "        final_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', final_model_instance)\n",
    "        ])\n",
    "    else:\n",
    "        final_model_instance = eval(f\"{best_overall_model_name}()\")\n",
    "        final_pipeline = Pipeline([\n",
    "            ('model', final_model_instance)\n",
    "        ])\n",
    "    \n",
    "    # Define os melhores parâmetros no pipeline\n",
    "    final_pipeline.set_params(**best_params)\n",
    "    \n",
    "    print(f\"\\n--- Avaliando o melhor modelo no conjunto de teste: {best_overall_model_name} ---\")\n",
    "    \n",
    "    # AVALIANDO O MODELO AQUI COM DADOS QUE ELE NUNCA VIU\n",
    "    y_test_pred = final_pipeline.predict(X_test_best)\n",
    "    r2_final = r2_score(y_test, y_test_pred)\n",
    "    rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    print(f\" - R²: {r2_final:.4f}\")\n",
    "    print(f\" - RMSE: {rmse_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66089542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Carregando e dividindo os dados ---\n",
      "\n",
      "--- Iniciando a Busca em Grade para o modelo: Lasso ---\n",
      "Fitting 60 folds for each of 10 candidates, totalling 600 fits\n",
      "Busca em Grade para Lasso Concluída.\n",
      "Melhores hiperparâmetros: {'model__alpha': np.float64(0.030000000000000006)}\n",
      "Melhor pontuação (RMSE) na validação cruzada: 0.3913\n",
      "\n",
      "--- Iniciando a Busca em Grade para o modelo: SVR ---\n",
      "Fitting 60 folds for each of 12 candidates, totalling 720 fits\n",
      "Busca em Grade para SVR Concluída.\n",
      "Melhores hiperparâmetros: {'model__C': 0.1, 'model__gamma': 'scale', 'model__kernel': 'linear'}\n",
      "Melhor pontuação (RMSE) na validação cruzada: 0.4040\n",
      "\n",
      "--- Iniciando a Busca em Grade para o modelo: DecisionTreeRegressor ---\n",
      "Fitting 60 folds for each of 60 candidates, totalling 3600 fits\n",
      "Busca em Grade para DecisionTreeRegressor Concluída.\n",
      "Melhores hiperparâmetros: {'model__max_depth': 5, 'model__min_samples_leaf': 20, 'model__min_samples_split': 2}\n",
      "Melhor pontuação (RMSE) na validação cruzada: 0.4526\n",
      "\n",
      "--- Iniciando a Busca em Grade para o modelo: RandomForestRegressor ---\n",
      "Fitting 60 folds for each of 324 candidates, totalling 19440 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     exit()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 2. Executar a busca em grade para todos os modelos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m best_results_df = \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURE_SETS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPARAM_GRIDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 3. Avaliar o melhor modelo geral no conjunto de teste\u001b[39;00m\n\u001b[32m     18\u001b[39m evaluate_final_model(best_results_df, X_test, y_test, FEATURE_SETS)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(X_train, y_train, groups_train, feature_sets, param_grids)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Execução do GridSearchCV\u001b[39;00m\n\u001b[32m     55\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     56\u001b[39m     pipeline,\n\u001b[32m     57\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     return_train_score=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     63\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBusca em Grade para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Concluída.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMelhores hiperparâmetros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/tcore/.venv/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Função Principal de Execução ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Carregar e dividir os dados\n",
    "    print(\"--- Carregando e dividindo os dados ---\")\n",
    "    X_train, X_test, y_train, y_test, groups_train, groups_test = load_and_split_data(\n",
    "        X_path='/home/leticia-gontijo/Documents/tcore/data/processed-data/X-data1-1km.csv',\n",
    "        y_path='/home/leticia-gontijo/Documents/tcore/data/processed-data/y-data1-1km.csv',\n",
    "        groups_path='/home/leticia-gontijo/Documents/tcore/data/processed-data/groups-data1-1km.csv'\n",
    "    )\n",
    "    if X_train is None:\n",
    "        exit()\n",
    "\n",
    "    # 2. Executar a busca em grade para todos os modelos\n",
    "    best_results_df = run_grid_search(X_train, y_train, groups_train, FEATURE_SETS, PARAM_GRIDS)\n",
    "\n",
    "    # 3. Avaliar o melhor modelo geral no conjunto de teste\n",
    "    evaluate_final_model(best_results_df, X_test, y_test, FEATURE_SETS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cae09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "# Conjuntos de features\n",
    "FEATURE_SETS = {\n",
    "    'DS1': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'column_Male', 'age', 'vo2máx'],\n",
    "    'DS2': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'age', 'column_Male', 'vo2máx', 'speed'],\n",
    "    'DS3': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'age', 'column_Male', 'vo2máx', 'speed', 'umidade_absoluta', 'dry_temp', 'wet_temp', 'relative_humidity']\n",
    "}\n",
    "\n",
    "\n",
    "# Grades de hiperparâmetros para cada modelo\n",
    "PARAM_GRIDS = {\n",
    "    'Lasso': {\n",
    "        'model__alpha': np.linspace(0.01, 0.1, 10),\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__kernel': ['rbf', 'linear'],\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'model__max_depth': [5, 10, 15, 20, None],\n",
    "        'model__min_samples_leaf': [1, 5, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'model__n_estimators': [100, 200, 400],\n",
    "        'model__max_depth': [5, 10, 15],\n",
    "        'model__min_samples_leaf': [5, 10, 20],\n",
    "        'model__max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
    "        'model__max_samples': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'XGBoostRegressor': {\n",
    "        'model__n_estimators': [300, 500, 700],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__subsample': [0.5, 0.7, 0.9],\n",
    "        'model__colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Funções para o Fluxo do Modelo ---\n",
    "\n",
    "def load_and_split_data(X_path, y_path, groups_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Carrega os dados e os divide em conjuntos de treino e teste.\n",
    "    Utiliza GroupShuffleSplit para garantir que os grupos não se misturem entre os conjuntos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X = pd.read_csv(X_path)\n",
    "        y = pd.read_csv(y_path)\n",
    "        groups = pd.read_csv(groups_path)['trial_number']\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro: Arquivo não encontrado - {e.filename}. Verifique se os caminhos estão corretos.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx].values.ravel(), y.iloc[test_idx].values.ravel()\n",
    "    groups_train = groups.iloc[train_idx]\n",
    "    groups_test = groups.iloc[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, groups_train, groups_test\n",
    "\n",
    "\n",
    "def run_grid_search(X_train, y_train, groups_train, feature_sets, param_grids):\n",
    "    \"\"\"\n",
    "    Executa a busca em grade para múltiplos modelos, retornando os melhores resultados.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    scoring = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
    "\n",
    "    for model_name, param_grid in param_grids.items():\n",
    "        print(f\"\\n--- Iniciando a Busca em Grade para o modelo: {model_name} ---\")\n",
    "\n",
    "        for ds_name, feature_list in feature_sets.items():\n",
    "            X_train_subset = X_train[feature_list]\n",
    "\n",
    "            # Configura o pipeline com o modelo e o scaler se necessário\n",
    "            if model_name in ['Lasso', 'SVR']:\n",
    "                model = eval(f\"{model_name}(\" + \"random_state=42)\" if model_name != \"SVR\" else f\"{model_name}()\")\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('model', model)\n",
    "                ])\n",
    "            else:\n",
    "                model = eval(f\"{model_name}(\" + \"random_state=42)\" if model_name != \"SVR\" else f\"{model_name}()\")\n",
    "                pipeline = Pipeline([\n",
    "                    ('model', model)\n",
    "                ])\n",
    "\n",
    "            # Execução do GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline,\n",
    "                param_grid=param_grid,\n",
    "                cv=LeaveOneGroupOut(),\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1,\n",
    "                verbose=0, # Alterado para 0 para não poluir o console com o log de cada busca\n",
    "                return_train_score=True\n",
    "            )\n",
    "            grid_search.fit(X_train_subset, y_train, groups=groups_train)\n",
    "            \n",
    "            # Captura as métricas do melhor estimador\n",
    "            best_estimator = grid_search.best_estimator_\n",
    "            y_train_pred = best_estimator.predict(X_train_subset)\n",
    "            \n",
    "            # Calcula as métricas de treino (na verdade, do conjunto de validação da melhor iteração)\n",
    "            best_train_score = grid_search.cv_results_['mean_train_score'][grid_search.best_index_]\n",
    "            best_val_score = grid_search.cv_results_['mean_test_score'][grid_search.best_index_]\n",
    "            \n",
    "            # Adiciona os resultados à lista\n",
    "            results_list.append({\n",
    "                'Modelo': model_name,\n",
    "                'Conjunto de Features': ds_name,\n",
    "                'Melhores Parâmetros': grid_search.best_params_,\n",
    "                'RMSE (Validação Cruzada)': -best_val_score,\n",
    "                'R² (Treino)': r2_score(y_train, y_train_pred),\n",
    "                'RMSE (Treino)': np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            })\n",
    "\n",
    "    return results_list\n",
    "\n",
    "\n",
    "def evaluate_final_models_on_test_set(results_list, X_test, y_test, feature_sets):\n",
    "    \"\"\"\n",
    "    Avalia cada modelo, com seus melhores parâmetros, no conjunto de teste.\n",
    "    \"\"\"\n",
    "    for result in results_list:\n",
    "        model_name = result['Modelo']\n",
    "        ds_name = result['Conjunto de Features']\n",
    "        best_params = result['Melhores Parâmetros']\n",
    "        feature_list = feature_sets[ds_name]\n",
    "        \n",
    "        X_test_subset = X_test[feature_list]\n",
    "\n",
    "        # Re-instancia o modelo com os melhores parâmetros e o pipeline\n",
    "        if model_name in ['Lasso', 'SVR']:\n",
    "            model = eval(f\"{model_name}()\")\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', model)\n",
    "            ])\n",
    "        else:\n",
    "            model = eval(f\"{model_name}()\")\n",
    "            pipeline = Pipeline([\n",
    "                ('model', model)\n",
    "            ])\n",
    "        \n",
    "        # Define os melhores parâmetros no pipeline\n",
    "        pipeline.set_params(**best_params)\n",
    "\n",
    "        # Treina o modelo final com todo o conjunto de treino para a avaliação de teste\n",
    "        # Encontra o resultado correspondente para obter os dados de treino\n",
    "        train_result = next((item for item in results_list if item['Modelo'] == model_name and item['Conjunto de Features'] == ds_name), None)\n",
    "        X_train_subset = X_train[train_result['Conjunto de Features']]\n",
    "        pipeline.fit(X_train_subset, y_train)\n",
    "\n",
    "        # Avaliação no conjunto de teste (hold-out)\n",
    "        y_test_pred = pipeline.predict(X_test_subset)\n",
    "        r2_final = r2_score(y_test, y_test_pred)\n",
    "        rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "        # Adiciona as métricas de teste ao dicionário de resultados\n",
    "        result['R² (Teste)'] = r2_final\n",
    "        result['RMSE (Teste)'] = rmse_final\n",
    "\n",
    "# --- Função Principal de Execução ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Carregar e dividir os dados\n",
    "    print(\"--- Carregando e dividindo os dados ---\")\n",
    "    X_train, X_test, y_train, y_test, groups_train, groups_test = load_and_split_data(\n",
    "        X_path='/home/leticia-gontijo/Documents/tcore/data/processed-data/X-data1-1km.csv',\n",
    "        y_path='/home/leticia-gontijo/Documents/tcore/data/processed-data/y-data1-1km.csv',\n",
    "        groups_path='/home/leticia-gontijo/Documents/tcore/data/processed-data/groups-data1-1km.csv'\n",
    "    )\n",
    "    if X_train is None:\n",
    "        exit()\n",
    "\n",
    "    # 2. Executar a busca em grade para todos os modelos e conjuntos de features\n",
    "    print(\"--- Executando a Busca em Grade ---\")\n",
    "    all_results = run_grid_search(X_train, y_train, groups_train, FEATURE_SETS, PARAM_GRIDS)\n",
    "\n",
    "    # 3. Avaliar cada modelo no conjunto de teste\n",
    "    print(\"\\n--- Avaliando os Modelos no Conjunto de Teste ---\")\n",
    "    evaluate_final_models_on_test_set(all_results, X_test, y_test, FEATURE_SETS)\n",
    "\n",
    "    # 4. Criar e exibir a tabela de resultados\n",
    "    print(\"\\n--- Tabela de Resultados Final ---\")\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Arredondar valores para melhor visualização\n",
    "    results_df['R² (Treino)'] = results_df['R² (Treino)'].round(4)\n",
    "    results_df['RMSE (Treino)'] = results_df['RMSE (Treino)'].round(4)\n",
    "    results_df['R² (Teste)'] = results_df['R² (Teste)'].round(4)\n",
    "    results_df['RMSE (Teste)'] = results_df['RMSE (Teste)'].round(4)\n",
    "    results_df['RMSE (Validação Cruzada)'] = results_df['RMSE (Validação Cruzada)'].round(4)\n",
    "    \n",
    "    # Reorganizar as colunas para melhor visualização\n",
    "    results_df = results_df[[\n",
    "        'Modelo', 'Conjunto de Features', 'R² (Treino)', 'RMSE (Treino)',\n",
    "        'RMSE (Validação Cruzada)', 'R² (Teste)', 'RMSE (Teste)', 'Melhores Parâmetros'\n",
    "    ]]\n",
    "    \n",
    "    # Exibir a tabela\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a12af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TESTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d736f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, GridSearchCV, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "# --- Definição de Constantes e Hiperparâmetros ---\n",
    "# Conjuntos de features\n",
    "FEATURE_SETS = {\n",
    "    'DS1': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'column_Male', 'age', 'vo2máx'],\n",
    "    'DS2': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'age', 'column_Male', 'vo2máx', 'speed'],\n",
    "    'DS3': ['kilometer', 'wbgt', 'skin_temp', 'heart_rate', 'age', 'column_Male', 'vo2máx', 'speed', 'umidade_absoluta', 'dry_temp', 'wet_temp', 'relative_humidity']\n",
    "}\n",
    "\n",
    "# Hiperparâmetros otimizados do notebook original (manter para referência)\n",
    "OPTIMIZED_PARAMS = {\n",
    "    'Lasso': {\n",
    "        'DS1': {'alpha': 0.02868},\n",
    "        'DS2': {'alpha': 0.02394},\n",
    "        'DS3': {'alpha': 0.02631}\n",
    "    },\n",
    "    'SVR': {\n",
    "        'DS1': {'kernel': 'rbf', 'C': 1.0, 'gamma': 'scale', 'epsilon': 0.1},\n",
    "        'DS2': {'kernel': 'rbf', 'C': 1.0, 'gamma': 'scale', 'epsilon': 0.1},\n",
    "        'DS3': {'kernel': 'rbf', 'C': 1.0, 'gamma': 'scale', 'epsilon': 0.1}\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'DS1': {'max_depth': 10, 'min_samples_leaf': 25, 'min_samples_split': 5},\n",
    "        'DS2': {'max_depth': 10, 'min_samples_leaf': 25, 'min_samples_split': 5},\n",
    "        'DS3': {'max_depth': 10, 'min_samples_leaf': 25, 'min_samples_split': 5}\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'DS1': {'max_features': 0.8, 'max_samples': 0.8, 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 200},\n",
    "        'DS2': {'max_features': 0.7, 'max_samples': 0.8, 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 200},\n",
    "        'DS3': {'max_features': 0.7, 'max_samples': 0.8, 'min_samples_leaf': 10, 'min_samples_split': 30, 'n_estimators': 400}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'DS1': {'colsample_bytree': 0.5, 'gamma': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 700, 'reg_alpha': 0.1, 'reg_lambda': 6, 'subsample': 0.5},\n",
    "        'DS2': {'colsample_bytree': 0.5, 'gamma': 1.0, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'reg_alpha': 0.1, 'reg_lambda': 8, 'subsample': 0.5},\n",
    "        'DS3': {'colsample_bytree': 0.5, 'gamma': 1.0, 'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 700, 'reg_alpha': 0.1, 'reg_lambda': 8, 'subsample': 0.5}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Grades de hiperparâmetros para cada modelo\n",
    "PARAM_GRIDS = {\n",
    "    'Lasso': {\n",
    "        'model__alpha': np.linspace(0.01, 0.1, 10),\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__C': [0.1, 1.0, 10.0],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__kernel': ['rbf', 'linear'],\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'model__max_depth': [5, 10, 15, 20, None],\n",
    "        'model__min_samples_leaf': [1, 5, 10, 20],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200, 400],\n",
    "        'model__max_depth': [5, 10, 15],\n",
    "        'model__min_samples_leaf': [5, 10, 20],\n",
    "        'model__max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
    "        'model__max_samples': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [300, 500, 700],\n",
    "        'model__max_depth': [3, 5, 7],\n",
    "        'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'model__subsample': [0.5, 0.7, 0.9],\n",
    "        'model__colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dicionário para mapear os nomes dos modelos para as suas classes\n",
    "MODELS_TO_RUN = {\n",
    "    'Lasso': Lasso(random_state=42, max_iter=20000),\n",
    "    'SVR': SVR(),\n",
    "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "}\n",
    "\n",
    "# --- Funções para o Fluxo do Modelo ---\n",
    "\n",
    "def load_and_split_data(X_path, y_path, groups_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Carrega os dados e os divide em conjuntos de treino e teste.\n",
    "    Utiliza GroupShuffleSplit para garantir que os grupos não se misturem entre os conjuntos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X = pd.read_csv(X_path)\n",
    "        y = pd.read_csv(y_path)\n",
    "        groups = pd.read_csv(groups_path)['trial_number']\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro: Arquivo não encontrado - {e.filename}. Verifique se os caminhos estão corretos.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups=groups))\n",
    "\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx].values.ravel(), y.iloc[test_idx].values.ravel()\n",
    "    groups_train = groups.iloc[train_idx]\n",
    "    groups_test = groups.iloc[test_idx]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, groups_train, groups_test\n",
    "\n",
    "\n",
    "def run_grid_search_and_evaluate(X_train, y_train, groups_train, X_test, y_test, feature_sets, param_grids, models_to_run):\n",
    "    \"\"\"\n",
    "    Executa a busca em grade para múltiplos modelos e conjuntos de features,\n",
    "    e avalia o melhor modelo de cada busca.\n",
    "    \"\"\"\n",
    "    best_results = []\n",
    "    scoring = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
    "\n",
    "    for model_name, model_instance in models_to_run.items():\n",
    "        print(f\"\\n--- Iniciando a Busca em Grade para o modelo: {model_name} ---\")\n",
    "\n",
    "        for feature_set_name, feature_list in feature_sets.items():\n",
    "            print(f\"\\n--- Otimizando com o conjunto de features: {feature_set_name} ---\")\n",
    "            \n",
    "            # Subconjunto dos dados com as features atuais\n",
    "            X_train_subset = X_train[feature_list]\n",
    "            X_test_subset = X_test[feature_list]\n",
    "\n",
    "            # Configura o pipeline com o scaler se necessário\n",
    "            if model_name in ['Lasso', 'SVR']:\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('model', model_instance)\n",
    "                ])\n",
    "            else:\n",
    "                pipeline = Pipeline([\n",
    "                    ('model', model_instance)\n",
    "                ])\n",
    "            \n",
    "            param_grid = param_grids.get(model_name)\n",
    "            if not param_grid:\n",
    "                print(f\"Aviso: Grade de hiperparâmetros não encontrada para {model_name}. Pulando este modelo.\")\n",
    "                continue\n",
    "\n",
    "            # Execução do GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline,\n",
    "                param_grid=param_grid,\n",
    "                cv=LeaveOneGroupOut(),\n",
    "                scoring=scoring,\n",
    "                n_jobs=-1,\n",
    "                verbose=1,\n",
    "                return_train_score=True\n",
    "            )\n",
    "            grid_search.fit(X_train_subset, y_train, groups=groups_train)\n",
    "\n",
    "            print(f\"Busca em Grade para {model_name} com {feature_set_name} Concluída.\")\n",
    "            print(f\"Melhores hiperparâmetros: {grid_search.best_params_}\")\n",
    "            print(f\"Melhor pontuação (RMSE) na validação cruzada: {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "            # Avaliação no conjunto de teste\n",
    "            y_test_pred = grid_search.best_estimator_.predict(X_test_subset)\n",
    "            r2_final = r2_score(y_test, y_test_pred)\n",
    "            rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "            print(f\" - R² no conjunto de teste: {r2_final:.4f}\")\n",
    "            print(f\" - RMSE no conjunto de teste: {rmse_final:.4f}\")\n",
    "\n",
    "            best_results.append({\n",
    "                'model': model_name,\n",
    "                'feature_set': feature_set_name, # Novo campo para identificar o conjunto de features\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_cv_score': -grid_search.best_score_,\n",
    "                'test_r2': r2_final,\n",
    "                'test_rmse': rmse_final\n",
    "            })\n",
    "     \n",
    "    return pd.DataFrame(best_results)\n",
    "\n",
    "\n",
    "# --- Função Principal de Execução ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Carregar e dividir os dados\n",
    "    print(\"--- Carregando e dividindo os dados ---\")\n",
    "    X_train, X_test, y_train, y_test, groups_train, groups_test = load_and_split_data(\n",
    "        X_path='/home/usuario-leticia/Desktop/Samuel/leticiaag/tcore/data/processed-data/X-data1-1km.csv',\n",
    "        y_path='/home/usuario-leticia/Desktop/Samuel/leticiaag/tcore/data/processed-data/y-data1-1km.csv',\n",
    "        groups_path='/home/usuario-leticia/Desktop/Samuel/leticiaag/tcore/data/processed-data/groups-data1-1km.csv'\n",
    "    )\n",
    "    # Garante que os dados foram carregados antes de prosseguir\n",
    "    if X_train is None:\n",
    "        exit()\n",
    "        \n",
    "    print(\"Dados carregados e divididos com sucesso.\")\n",
    "\n",
    "    # 2. Executar a busca em grade para todos os modelos\n",
    "    print(\"\\n--- Iniciando a Busca em Grade para todos os algoritmos ---\")\n",
    "    results_df = run_grid_search_and_evaluate(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        groups_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        FEATURE_SETS,\n",
    "        PARAM_GRIDS,\n",
    "        MODELS_TO_RUN\n",
    "    )\n",
    "\n",
    "    # 3. Exibir os resultados finais\n",
    "    print(\"\\n--- Resumo dos Melhores Resultados ---\")\n",
    "    print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa47789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
